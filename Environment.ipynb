{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class VacEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VacEnv, self).__init__()\n",
    "        \n",
    "        # Define action space: continuous velocity adjustments in 2D\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(2,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space (noisy measurements and detected entities)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"vac_position\": spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32),\n",
    "            \"vac_velocity\": spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32),\n",
    "            \"obstacle_positions\": spaces.Box(low=-np.inf, high=np.inf, shape=(1, 2), dtype=np.float32),  # 1 obstacle\n",
    "            \"radar_positions\": spaces.Box(low=-np.inf, high=np.inf, shape=(3, 2), dtype=np.float32),  # Max 3 Radons\n",
    "            \"goal_position\": spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32),\n",
    "            \"detected\": spaces.Discrete(2)\n",
    "        })\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.noise_std = 0.1  # Noise for observations\n",
    "        self.r_vac = 5.0      # Radar radius of the vac\n",
    "        self.max_speed = 5.0  # Maximum velocity\n",
    "        \n",
    "        # Initialize state variables in reset()\n",
    "        self.pos_vac = None\n",
    "        self.vel_vac = None\n",
    "        self.ebs = None       # Obstacle positions\n",
    "        self.pos_radons = None# Radon positions\n",
    "        self.r_radons = None  # Radon radii\n",
    "        self.pos_grad = None  # Goal position\n",
    "        self.detected_flag = False\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Initialize state\n",
    "        self.pos_vac = np.array([0.0, 0.0], dtype=np.float32)  # Initial position (pos_vatt)\n",
    "        self.vel_vac = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.ebs = np.array([[10.0, 10.0]], dtype=np.float32)  # Single obstacle\n",
    "        self.pos_radons = np.array(                             # Three Radons\n",
    "            [[15.0, 15.0], [20.0, 20.0], [25.0, 25.0]], dtype=np.float32\n",
    "        )\n",
    "        self.r_radons = np.array([3.0, 3.0, 3.0], dtype=np.float32)\n",
    "        self.pos_grad = np.array([30.0, 30.0], dtype=np.float32)  # Goal (pos_grad)\n",
    "        self.detected_flag = False\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Add noise to vac's position and velocity\n",
    "        noisy_pos = self.pos_vac + np.random.normal(0, self.noise_std, size=2)\n",
    "        noisy_vel = self.vel_vac + np.random.normal(0, self.noise_std, size=2)\n",
    "        \n",
    "        # Detect Radons within radar radius (r_vac)\n",
    "        radar_positions = []\n",
    "        for radon_pos in self.pos_radons:\n",
    "            distance = np.linalg.norm(self.pos_vac - radon_pos)\n",
    "            if distance <= self.r_vac:\n",
    "                radar_positions.append(radon_pos)\n",
    "        # Pad with zeros if fewer than 3 Radons detected\n",
    "        radar_positions += [np.zeros(2)] * (3 - len(radar_positions))\n",
    "        radar_positions = np.array(radar_positions, dtype=np.float32)\n",
    "        \n",
    "        # Check if detected by any Radon\n",
    "        detected = False\n",
    "        for radon_pos, radon_r in zip(self.pos_radons, self.r_radons):\n",
    "            distance = np.linalg.norm(self.pos_vac - radon_pos)\n",
    "            if distance <= radon_r:\n",
    "                detected = True\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            \"vac_position\": noisy_pos,\n",
    "            \"vac_velocity\": noisy_vel,\n",
    "            \"obstacle_positions\": self.ebs,\n",
    "            \"radar_positions\": radar_positions,\n",
    "            \"goal_position\": self.pos_grad,\n",
    "            \"detected\": int(detected)\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update velocity (clipped to max_speed)\n",
    "        self.vel_vac = np.clip(self.vel_vac + action, -self.max_speed, self.max_speed)\n",
    "        # Update position\n",
    "        self.pos_vac += self.vel_vac\n",
    "        \n",
    "        # Check termination conditions\n",
    "        done = False\n",
    "        reward = -np.linalg.norm(self.pos_vac - self.pos_grad)  # Reward based on distance to goal\n",
    "        \n",
    "        # Collision with obstacle\n",
    "        for obs in self.ebs:\n",
    "            if np.linalg.norm(self.pos_vac - obs) < 1.0:  # Obstacle radius = 1.0\n",
    "                reward -= 50\n",
    "                done = True\n",
    "        \n",
    "        # Collision with Radon or detection\n",
    "        for radon_pos, radon_r in zip(self.pos_radons, self.r_radons):\n",
    "            if np.linalg.norm(self.pos_vac - radon_pos) < radon_r:\n",
    "                reward -= 50\n",
    "                done = True\n",
    "        \n",
    "        # Reached goal\n",
    "        if np.linalg.norm(self.pos_vac - self.pos_grad) < 2.0:\n",
    "            reward += 100\n",
    "            done = True\n",
    "        \n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass  # Optional: Add visualization logic\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      5\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# Replace with agent's policy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[3], line 109\u001b[0m, in \u001b[0;36mVacEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    106\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Reached goal\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_vac\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_grad\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2.0\u001b[39m:\n\u001b[0;32m    110\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    111\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\envs\\RL_Q\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2744\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2742\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2744\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdot(x)\n\u001b[0;32m   2745\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = VacEnv()\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Replace with agent's policy\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
